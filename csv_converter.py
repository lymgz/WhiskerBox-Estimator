#!/usr/bin/env python3
"""
CSVæ‰¹é‡ç®±çº¿å›¾æ•°æ®è½¬æ¢å·¥å…·
æ”¯æŒåŠ¨æ€åˆ—æ•°å’Œæœ€ä¸åˆ©æƒ…å†µåˆ†æ
"""

import csv
import os
import sys
import json
import pandas as pd
import math
from datetime import datetime
from typing import Dict, List, Tuple, Optional
from statistical_converter import StatisticalConverter

class CSVConverter:
    def __init__(self):
        self.converter = StatisticalConverter()
        self.template_filename = "template.csv"
        self.data_filename = "data.csv"
        
    def generate_template(self, situations_count: int = 4) -> str:
        """ç”ŸæˆCSVæ¨¡æ¿æ–‡ä»¶"""
        template_content = []
        
        # åŸºçº¿ç»„
        header = ["Baseline"] + [f"Case{i+1}" for i in range(situations_count)]
        template_content.append(header)
        
        data_items = ["Upper_Outlier", "Upper_Whisker", "Q3", "Q2", "Q1", "Lower_Whisker", "Lower_Outlier", "Sample_Size"]
        for item in data_items:
            row = [item] + [""] * situations_count
            template_content.append(row)
        
        # ç©ºè¡Œåˆ†éš”
        template_content.append([""] * (situations_count + 1))
        
        # å¹²é¢„ç»„
        header = ["Intervention"] + [f"Case{i+1}" for i in range(situations_count)]
        template_content.append(header)
        
        for item in data_items:
            row = [item] + [""] * situations_count
            template_content.append(row)
        
        # å†™å…¥æ–‡ä»¶
        with open(self.template_filename, 'w', newline='', encoding='utf-8') as f:
            writer = csv.writer(f)
            writer.writerows(template_content)
        
        return self.template_filename
    
    def read_csv_data(self, filename: str) -> Dict:
        """è¯»å–CSVæ•°æ®å¹¶è§£æ"""
        if not os.path.exists(filename):
            raise FileNotFoundError(f"æ‰¾ä¸åˆ°æ–‡ä»¶: {filename}")
        
        with open(filename, 'r', encoding='utf-8') as f:
            reader = csv.reader(f)
            rows = list(reader)
        
        return self._parse_csv_structure(rows)
    
    def _parse_csv_structure(self, rows: List[List[str]]) -> Dict:
        """è§£æCSVç»“æ„ï¼Œæ”¯æŒåŠ¨æ€åˆ—æ•°"""
        groups = {}
        current_group = None
        current_data = {}
        
        for row in rows:
            if not row or all(cell.strip() == "" for cell in row):
                # ç©ºè¡Œï¼Œç»“æŸå½“å‰ç»„
                if current_group and current_data:
                    groups[current_group] = current_data
                    current_data = {}
                continue
            
            first_cell = row[0].strip()
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯ç»„æ ‡é¢˜ (æ”¯æŒä¸­è‹±æ–‡)
            if first_cell in ["åŸºçº¿ç»„", "å¹²é¢„ç»„", "Baseline", "Intervention"] or first_cell.endswith("ç»„"):
                if current_group and current_data:
                    groups[current_group] = current_data
                
                current_group = first_cell
                current_data = {}
                
                # æ£€æµ‹æƒ…å†µæ•°é‡
                situations = [cell.strip() for cell in row[1:] if cell.strip()]
                current_data['situations'] = situations
                current_data['situation_count'] = len(situations)
                current_data['data'] = {}
                continue
            
            # æ•°æ®è¡Œ (æ”¯æŒä¸­è‹±æ–‡æ ‡ç­¾)
            chinese_labels = ["ä¸Šå¼‚å¸¸å€¼", "ä¸Šé¡»", "Q3", "Q2", "Q1", "ä¸‹é¡»", "ä¸‹å¼‚å¸¸å€¼", "æ ·æœ¬é‡"]
            english_labels = ["Upper_Outlier", "Upper_Whisker", "Q3", "Q2", "Q1", "Lower_Whisker", "Lower_Outlier", "Sample_Size"]
            
            if current_group and first_cell in chinese_labels + english_labels:
                # æ ‡å‡†åŒ–æ ‡ç­¾ä¸ºä¸­æ–‡ï¼ˆå†…éƒ¨å¤„ç†ç»Ÿä¸€ç”¨ä¸­æ–‡ï¼‰
                label_mapping = {
                    "Upper_Outlier": "ä¸Šå¼‚å¸¸å€¼",
                    "Upper_Whisker": "ä¸Šé¡»", 
                    "Lower_Whisker": "ä¸‹é¡»",
                    "Lower_Outlier": "ä¸‹å¼‚å¸¸å€¼",
                    "Sample_Size": "æ ·æœ¬é‡"
                }
                normalized_label = label_mapping.get(first_cell, first_cell)
                values = []
                for cell in row[1:]:
                    cell_value = cell.strip()
                    if cell_value:
                        try:
                            values.append(float(cell_value))
                        except ValueError:
                            values.append(None)
                    else:
                        values.append(None)
                
                current_data['data'][normalized_label] = values
        
        # å¤„ç†æœ€åä¸€ç»„
        if current_group and current_data:
            groups[current_group] = current_data
        
        return groups
    
    def analyze_data_levels(self, groups: Dict) -> Dict:
        """åˆ†ææ¯ä¸ªæƒ…å†µçš„æ•°æ®ç­‰çº§"""
        analysis = {}
        
        for group_name, group_data in groups.items():
            situations_analysis = []
            situation_count = group_data.get('situation_count', 0)
            
            for i in range(situation_count):
                situation_data = self._extract_situation_data(group_data, i)
                level = self._determine_data_level(situation_data)
                
                situations_analysis.append({
                    'situation_index': i + 1,
                    'situation_name': group_data['situations'][i] if i < len(group_data['situations']) else f"æƒ…å†µ{i+1}",
                    'data_level': level,
                    'available_data': self._list_available_data(situation_data),
                    'data': situation_data
                })
            
            # æ‰¾å‡ºæœ€ä¸åˆ©æƒ…å†µï¼ˆæœ€ä½ç­‰çº§ï¼‰
            min_level = min([s['data_level'] for s in situations_analysis]) if situations_analysis else 0
            
            analysis[group_name] = {
                'situations': situations_analysis,
                'min_level': min_level,
                'situation_count': situation_count,
                'conservative_strategy': min_level < max([s['data_level'] for s in situations_analysis]) if situations_analysis else False
            }
        
        return analysis
    
    def _extract_situation_data(self, group_data: Dict, situation_index: int) -> Dict:
        """æå–ç‰¹å®šæƒ…å†µçš„æ•°æ®"""
        situation_data = {}
        
        for data_type, values in group_data['data'].items():
            if situation_index < len(values) and values[situation_index] is not None:
                situation_data[data_type] = values[situation_index]
        
        return situation_data
    
    def _determine_data_level(self, situation_data: Dict) -> int:
        """ç¡®å®šæ•°æ®ç­‰çº§"""
        required_basic = ['Q1', 'Q2', 'Q3', 'æ ·æœ¬é‡']
        level1_additions = ['ä¸Šé¡»', 'ä¸‹é¡»']
        level2_additions = ['ä¸Šå¼‚å¸¸å€¼', 'ä¸‹å¼‚å¸¸å€¼']
        
        # æ£€æŸ¥åŸºç¡€æ•°æ®
        if not all(key in situation_data for key in required_basic):
            return -1  # æ•°æ®ä¸å®Œæ•´
        
        # æ£€æŸ¥ç­‰çº§1æ•°æ®
        has_whiskers = any(key in situation_data for key in level1_additions)
        
        # æ£€æŸ¥ç­‰çº§2æ•°æ®
        has_outliers = any(key in situation_data for key in level2_additions)
        
        if has_outliers and has_whiskers:
            return 2
        elif has_whiskers:
            return 1
        else:
            return 0
    
    def _list_available_data(self, situation_data: Dict) -> List[str]:
        """åˆ—å‡ºå¯ç”¨çš„æ•°æ®é¡¹"""
        return list(situation_data.keys())
    
    def convert_csv_data(self, filename: str, verbose: bool = False) -> Dict:
        """è½¬æ¢CSVæ•°æ®"""
        # è¯»å–æ•°æ®
        groups = self.read_csv_data(filename)
        
        # åˆ†ææ•°æ®ç­‰çº§
        analysis = self.analyze_data_levels(groups)
        
        # æ‰§è¡Œè½¬æ¢
        results = {}
        
        for group_name, group_analysis in analysis.items():
            group_results = []
            min_level = group_analysis['min_level']
            
            if verbose:
                print(f"\n=== {group_name} åˆ†æ ===")
                print(f"æ£€æµ‹åˆ° {group_analysis['situation_count']} ä¸ªæƒ…å†µ")
                print(f"æœ€ä¸åˆ©ç­‰çº§: {min_level}")
                if group_analysis['conservative_strategy']:
                    print("âš ï¸  é‡‡ç”¨ä¿å®ˆä¼°è®¡ç­–ç•¥")
            
            for situation in group_analysis['situations']:
                situation_data = situation['data']
                
                if situation_data.get('æ ·æœ¬é‡') is None:
                    if verbose:
                        print(f"è·³è¿‡ {situation['situation_name']}: ç¼ºå°‘æ ·æœ¬é‡")
                    continue
                
                # åˆ›å»ºç®±çº¿å›¾æ•°æ®
                boxplot_data = self._create_boxplot_data(situation_data, min_level)
                
                # è½¬æ¢
                try:
                    result = self.converter.convert_boxplot_to_stats(
                        boxplot_data, 
                        int(situation_data['æ ·æœ¬é‡']), 
                        'auto'
                    )
                    
                    result['situation_name'] = situation['situation_name']
                    result['original_level'] = situation['data_level']
                    result['used_level'] = min_level
                    result['conservative_estimate'] = situation['data_level'] > min_level
                    
                    group_results.append(result)
                    
                    if verbose:
                        print(f"{situation['situation_name']}: Mean={result['mean']:.3f}, SD={result['sd']:.3f} (ç­‰çº§{min_level})")
                
                except Exception as e:
                    if verbose:
                        print(f"è½¬æ¢å¤±è´¥ {situation['situation_name']}: {e}")
            
            results[group_name] = group_results
        
        return {
            'results': results,
            'analysis': analysis,
            'summary': self._generate_summary(analysis, results)
        }
    
    def _create_boxplot_data(self, situation_data: Dict, target_level: int) -> Dict:
        """æ ¹æ®ç›®æ ‡ç­‰çº§åˆ›å»ºç®±çº¿å›¾æ•°æ®"""
        boxplot_data = {
            'q1': situation_data.get('Q1'),
            'q2': situation_data.get('Q2'),
            'q3': situation_data.get('Q3')
        }
        
        if target_level >= 1:
            if 'ä¸Šé¡»' in situation_data:
                boxplot_data['upper_whisker'] = situation_data['ä¸Šé¡»']
            if 'ä¸‹é¡»' in situation_data:
                boxplot_data['lower_whisker'] = situation_data['ä¸‹é¡»']
        
        if target_level >= 2:
            outliers = []
            if 'ä¸Šå¼‚å¸¸å€¼' in situation_data:
                outliers.append(situation_data['ä¸Šå¼‚å¸¸å€¼'])
            if 'ä¸‹å¼‚å¸¸å€¼' in situation_data:
                outliers.append(situation_data['ä¸‹å¼‚å¸¸å€¼'])
            if outliers:
                boxplot_data['outliers'] = outliers
        
        return boxplot_data
    
    def _generate_summary(self, analysis: Dict, results: Dict) -> Dict:
        """ç”Ÿæˆåˆ†ææ‘˜è¦"""
        total_situations = sum(group['situation_count'] for group in analysis.values())
        conservative_groups = sum(1 for group in analysis.values() if group['conservative_strategy'])
        
        min_levels = [group['min_level'] for group in analysis.values()]
        overall_min_level = min(min_levels) if min_levels else 0
        
        return {
            'total_groups': len(analysis),
            'total_situations': total_situations,
            'conservative_groups': conservative_groups,
            'overall_min_level': overall_min_level,
            'precision_estimate': self._get_precision_description(overall_min_level),
            'recommendations': self._generate_recommendations(analysis)
        }
    
    def _get_precision_description(self, level: int) -> str:
        """è·å–ç²¾åº¦æè¿°"""
        descriptions = {
            -1: "æ•°æ®ä¸å®Œæ•´",
            0: "ä¸­ç­‰ç²¾åº¦ (è¯¯å·®15-25%)",
            1: "é«˜ç²¾åº¦ (è¯¯å·®8-15%)",
            2: "æœ€é«˜ç²¾åº¦ (è¯¯å·®5-10%)"
        }
        return descriptions.get(level, "æœªçŸ¥ç²¾åº¦")
    
    def _generate_recommendations(self, analysis: Dict) -> List[str]:
        """ç”Ÿæˆæ”¹è¿›å»ºè®®"""
        recommendations = []
        
        for group_name, group_analysis in analysis.items():
            if group_analysis['conservative_strategy']:
                low_level_situations = [
                    s['situation_name'] for s in group_analysis['situations'] 
                    if s['data_level'] == group_analysis['min_level']
                ]
                recommendations.append(
                    f"{group_name}: è¡¥å……{', '.join(low_level_situations)}çš„é¡»æ•°æ®å¯æå‡æ•´ä½“ç²¾åº¦"
                )
        
        return recommendations
    
    def calculate_group_comparison(self, group1_data: Dict, group2_data: Dict, 
                                 confidence_level: float = 0.95) -> Dict:
        """
        è®¡ç®—ä¸¤ç»„é—´çš„å·®å¼‚åˆ†æ
        åŸºäºç”¨æˆ·æä¾›çš„å…¬å¼ï¼š
        Î”Mean = Meanâ‚ - Meanâ‚‚
        SD_diff = âˆš(SDâ‚Â²/nâ‚ + SDâ‚‚Â²/nâ‚‚)
        CI = Î”Mean Â± zÂ·SD_diff
        """
        delta_mean = group1_data['mean'] - group2_data['mean']
        
        # è®¡ç®—å·®å¼‚çš„æ ‡å‡†å·®
        sd_diff = math.sqrt(
            (group1_data['sd']**2 / group1_data['sample_size']) + 
            (group2_data['sd']**2 / group2_data['sample_size'])
        )
        
        # è®¡ç®—ç½®ä¿¡åŒºé—´
        z_score = self._get_z_score(confidence_level)
        ci_lower = delta_mean - z_score * sd_diff
        ci_upper = delta_mean + z_score * sd_diff
        
        # è®¡ç®—æ•ˆåº”é‡ (Cohen's d)
        pooled_sd = math.sqrt(
            ((group1_data['sample_size'] - 1) * group1_data['sd']**2 + 
             (group2_data['sample_size'] - 1) * group2_data['sd']**2) / 
            (group1_data['sample_size'] + group2_data['sample_size'] - 2)
        )
        cohens_d = delta_mean / pooled_sd if pooled_sd > 0 else 0
        
        # Hedges' g (åå·®æ ¡æ­£çš„Cohen's d)
        correction_factor = 1 - (3 / (4 * (group1_data['sample_size'] + group2_data['sample_size']) - 9))
        hedges_g = cohens_d * correction_factor
        
        # è®¡ç®—på€¼ (åŒå°¾tæ£€éªŒ)
        df = group1_data['sample_size'] + group2_data['sample_size'] - 2
        t_stat = delta_mean / sd_diff if sd_diff > 0 else 0
        p_value = self._calculate_p_value(abs(t_stat), df)
        
        # åˆ¤æ–­æ˜¾è‘—æ€§
        alpha = 1 - confidence_level
        significant = p_value < alpha
        
        return {
            'delta_mean': round(delta_mean, 4),
            'sd_diff': round(sd_diff, 4),
            'ci_lower': round(ci_lower, 4),
            'ci_upper': round(ci_upper, 4),
            'confidence_level': confidence_level,
            'cohens_d': round(cohens_d, 4),
            'hedges_g': round(hedges_g, 4),
            'p_value': round(p_value, 4),
            'significant': significant,
            't_statistic': round(t_stat, 4),
            'degrees_of_freedom': df,
            'interpretation': self._interpret_comparison(delta_mean, ci_lower, ci_upper, significant)
        }
    
    def _get_z_score(self, confidence_level: float) -> float:
        """è·å–å¯¹åº”ç½®ä¿¡æ°´å¹³çš„zåˆ†æ•°"""
        z_scores = {
            0.90: 1.645,
            0.95: 1.96,
            0.99: 2.576
        }
        return z_scores.get(confidence_level, 1.96)
    
    def _calculate_p_value(self, t_stat: float, df: int) -> float:
        """ç®€åŒ–çš„på€¼è®¡ç®—ï¼ˆåŒå°¾æ£€éªŒï¼‰"""
        # è¿™æ˜¯ä¸€ä¸ªç®€åŒ–çš„å®ç°ï¼Œå®é™…åº”ç”¨ä¸­å¯èƒ½éœ€è¦æ›´ç²¾ç¡®çš„ç»Ÿè®¡å‡½æ•°
        if df <= 0:
            return 1.0
        
        # ä½¿ç”¨è¿‘ä¼¼å…¬å¼
        if t_stat == 0:
            return 1.0
        elif t_stat > 4:
            return 0.0001
        elif t_stat > 3:
            return 0.01
        elif t_stat > 2:
            return 0.05
        elif t_stat > 1.5:
            return 0.1
        else:
            return 0.2
    
    def _interpret_comparison(self, delta_mean: float, ci_lower: float, 
                            ci_upper: float, significant: bool) -> str:
        """è§£é‡Šæ¯”è¾ƒç»“æœ"""
        if significant:
            if ci_lower > 0:
                return "æ˜¾è‘—å·®å¼‚ï¼šç»„1æ˜¾è‘—é«˜äºç»„2"
            elif ci_upper < 0:
                return "æ˜¾è‘—å·®å¼‚ï¼šç»„1æ˜¾è‘—ä½äºç»„2"
            else:
                return "æ˜¾è‘—å·®å¼‚ï¼šä½†ç½®ä¿¡åŒºé—´åŒ…å«0"
        else:
            return "æ— æ˜¾è‘—å·®å¼‚"
    
    def perform_group_comparisons(self, result_data: Dict, comparison_type: str = "all", 
                                confidence_level: float = 0.95) -> Dict:
        """
        æ‰§è¡Œç»„é—´æ¯”è¾ƒåˆ†æ
        comparison_type: "all", "intervention-baseline", "pairwise"
        """
        results = result_data['results']
        comparisons = []
        
        if comparison_type in ["all", "intervention-baseline"]:
            # å¹²é¢„ç»„ vs åŸºçº¿ç»„æ¯”è¾ƒ
            baseline_results = results.get('Baseline', results.get('åŸºçº¿ç»„', []))
            intervention_results = results.get('Intervention', results.get('å¹²é¢„ç»„', []))
            
            # æŒ‰Caseé…å¯¹æ¯”è¾ƒ
            for i, baseline in enumerate(baseline_results):
                if i < len(intervention_results):
                    intervention = intervention_results[i]
                    comparison = self.calculate_group_comparison(
                        intervention, baseline, confidence_level
                    )
                    comparison.update({
                        'comparison_id': f"Intervention_vs_Baseline_{baseline['situation_name']}",
                        'group1_name': 'Intervention',
                        'group2_name': 'Baseline',
                        'case_name': baseline['situation_name'],
                        'group1_data': intervention,
                        'group2_data': baseline
                    })
                    comparisons.append(comparison)
        
        if comparison_type in ["all", "pairwise"]:
            # åŒç»„å†…Caseä¹‹é—´çš„ä¸¤ä¸¤æ¯”è¾ƒ
            for group_name, group_results in results.items():
                for i, case1 in enumerate(group_results):
                    for j, case2 in enumerate(group_results[i+1:], i+1):
                        comparison = self.calculate_group_comparison(
                            case1, case2, confidence_level
                        )
                        comparison.update({
                            'comparison_id': f"{group_name}_{case1['situation_name']}_vs_{case2['situation_name']}",
                            'group1_name': f"{group_name}_{case1['situation_name']}",
                            'group2_name': f"{group_name}_{case2['situation_name']}",
                            'case_name': f"{case1['situation_name']}_vs_{case2['situation_name']}",
                            'group1_data': case1,
                            'group2_data': case2
                        })
                        comparisons.append(comparison)
        
        return {
            'comparisons': comparisons,
            'comparison_type': comparison_type,
            'confidence_level': confidence_level,
            'total_comparisons': len(comparisons),
            'significant_comparisons': sum(1 for c in comparisons if c['significant'])
        }
    
    def generate_meta_analysis_formats(self, result_data: Dict, comparison_data: Dict, 
                                     output_dir: str = "results") -> Dict[str, str]:
        """ç”Ÿæˆå¤šç§Metaåˆ†ææ ‡å‡†æ ¼å¼"""
        self.ensure_results_dir(output_dir)
        saved_files = {}
        
        # é€šç”¨Metaåˆ†ææ ¼å¼
        universal_data = self._create_universal_meta_format(result_data, comparison_data)
        universal_path = os.path.join(output_dir, "meta_universal.csv")
        universal_path = self.get_available_filename(universal_path)
        
        df_universal = pd.DataFrame(universal_data)
        df_universal.to_csv(universal_path, index=False, encoding='utf-8')
        saved_files['universal'] = universal_path
        
        # RevManæ ¼å¼
        revman_data = self._create_revman_format(result_data)
        revman_path = os.path.join(output_dir, "meta_revman.csv")
        revman_path = self.get_available_filename(revman_path)
        
        df_revman = pd.DataFrame(revman_data)
        df_revman.to_csv(revman_path, index=False, encoding='utf-8')
        saved_files['revman'] = revman_path
        
        # R MetaåŒ…æ ¼å¼
        r_meta_data = self._create_r_meta_format(comparison_data)
        r_path = os.path.join(output_dir, "meta_r.csv")
        r_path = self.get_available_filename(r_path)
        
        df_r = pd.DataFrame(r_meta_data)
        df_r.to_csv(r_path, index=False, encoding='utf-8')
        saved_files['r_meta'] = r_path
        
        return saved_files
    
    def _create_universal_meta_format(self, result_data: Dict, comparison_data: Dict) -> List[Dict]:
        """åˆ›å»ºé€šç”¨Metaåˆ†ææ ¼å¼"""
        universal_data = []
        
        for comparison in comparison_data['comparisons']:
            if 'Intervention_vs_Baseline' in comparison['comparison_id']:
                universal_data.append({
                    'Study_ID': comparison['case_name'],
                    'Comparison_Type': 'Intervention-Baseline',
                    'Intervention_Mean': comparison['group1_data']['mean'],
                    'Intervention_SD': comparison['group1_data']['sd'],
                    'Intervention_N': comparison['group1_data']['sample_size'],
                    'Control_Mean': comparison['group2_data']['mean'],
                    'Control_SD': comparison['group2_data']['sd'],
                    'Control_N': comparison['group2_data']['sample_size'],
                    'Mean_Difference': comparison['delta_mean'],
                    'SD_Difference': comparison['sd_diff'],
                    'Effect_Size_Cohens_d': comparison['cohens_d'],
                    'Effect_Size_Hedges_g': comparison['hedges_g'],
                    'SE_Mean_Diff': comparison['sd_diff'],
                    '95_CI_Lower': comparison['ci_lower'],
                    '95_CI_Upper': comparison['ci_upper'],
                    'P_Value': comparison['p_value'],
                    'Significant': 'Yes' if comparison['significant'] else 'No',
                    'Data_Quality_Level': comparison['group1_data']['used_level'],
                    'Conservative_Estimate': 'Yes' if comparison['group1_data'].get('conservative_estimate') else 'No',
                    'Original_Method': comparison['group1_data']['method_used'],
                    'Notes': comparison['interpretation']
                })
        
        return universal_data
    
    def _create_revman_format(self, result_data: Dict) -> List[Dict]:
        """åˆ›å»ºRevManæ ¼å¼"""
        revman_data = []
        results = result_data['results']
        
        baseline_results = results.get('Baseline', results.get('åŸºçº¿ç»„', []))
        intervention_results = results.get('Intervention', results.get('å¹²é¢„ç»„', []))
        
        for i, baseline in enumerate(baseline_results):
            if i < len(intervention_results):
                intervention = intervention_results[i]
                revman_data.append({
                    'Study_ID': baseline['situation_name'],
                    'Intervention_Mean': intervention['mean'],
                    'Intervention_SD': intervention['sd'],
                    'Intervention_N': intervention['sample_size'],
                    'Control_Mean': baseline['mean'],
                    'Control_SD': baseline['sd'],
                    'Control_N': baseline['sample_size']
                })
        
        return revman_data
    
    def _create_r_meta_format(self, comparison_data: Dict) -> List[Dict]:
        """åˆ›å»ºR MetaåŒ…æ ¼å¼"""
        r_data = []
        
        for comparison in comparison_data['comparisons']:
            if 'Intervention_vs_Baseline' in comparison['comparison_id']:
                r_data.append({
                    'Study': comparison['case_name'],
                    'TE': comparison['delta_mean'],
                    'seTE': comparison['sd_diff'],
                    'n.e': comparison['group1_data']['sample_size'],
                    'n.c': comparison['group2_data']['sample_size'],
                    'mean.e': comparison['group1_data']['mean'],
                    'sd.e': comparison['group1_data']['sd'],
                    'mean.c': comparison['group2_data']['mean'],
                    'sd.c': comparison['group2_data']['sd']
                })
        
        return r_data
    
    def is_file_locked(self, filepath: str) -> bool:
        """æ£€æµ‹æ–‡ä»¶æ˜¯å¦è¢«å ç”¨"""
        if not os.path.exists(filepath):
            return False
        
        try:
            with open(filepath, 'a'):
                return False
        except (IOError, OSError):
            return True
    
    def get_available_filename(self, base_filename: str) -> str:
        """è·å–å¯ç”¨çš„æ–‡ä»¶åï¼Œå¤„ç†æ–‡ä»¶å ç”¨æƒ…å†µ"""
        # åˆ†ç¦»æ–‡ä»¶åå’Œæ‰©å±•å
        name, ext = os.path.splitext(base_filename)
        
        # é¦–å…ˆå°è¯•åŸå§‹æ–‡ä»¶å
        if not self.is_file_locked(base_filename):
            return base_filename
        
        # å°è¯• _01 åˆ° _99
        for i in range(1, 100):
            suffix = f"_{i:02d}"
            new_filename = f"{name}{suffix}{ext}"
            if not self.is_file_locked(new_filename):
                return new_filename
        
        # è¶…è¿‡99ï¼Œä»_01å¼€å§‹è¦†ç›–
        return f"{name}_01{ext}"
    
    def ensure_results_dir(self, output_dir: str = "results") -> str:
        """ç¡®ä¿ç»“æœç›®å½•å­˜åœ¨"""
        if not os.path.exists(output_dir):
            os.makedirs(output_dir)
        return output_dir
    
    def save_to_excel(self, result_data: Dict, comparison_data: Dict = None, 
                     output_dir: str = "results", base_filename: str = "iqr_results.xlsx") -> str:
        """ä¿å­˜ç»“æœåˆ°Excelæ–‡ä»¶"""
        self.ensure_results_dir(output_dir)
        filepath = os.path.join(output_dir, base_filename)
        final_filepath = self.get_available_filename(filepath)
        
        # åˆ›å»ºExcelå†™å…¥å™¨
        with pd.ExcelWriter(final_filepath, engine='openpyxl') as writer:
            # Sheet1: è½¬æ¢ç»“æœ
            results_data = []
            for group_name, group_results in result_data['results'].items():
                for result in group_results:
                    results_data.append({
                        'Group': group_name,
                        'Case': result['situation_name'],
                        'Mean': round(result['mean'], 4),
                        'SD': round(result['sd'], 4),
                        'Sample_Size': result['sample_size'],
                        'Data_Level': result['used_level'],
                        'Method': result['method_used'],
                        'Conservative_Estimate': 'Yes' if result.get('conservative_estimate') else 'No',
                        'Precision': result.get('precision_estimate', ''),
                        'Formula_Source': result.get('formula_source', '')
                    })
            
            if results_data:
                df_results = pd.DataFrame(results_data)
                df_results.to_excel(writer, sheet_name='è½¬æ¢ç»“æœ', index=False)
            
            # Sheet2: ç»„é—´æ¯”è¾ƒç»“æœ (å¦‚æœæœ‰æ¯”è¾ƒæ•°æ®)
            if comparison_data and comparison_data['comparisons']:
                comparison_results = []
                for comp in comparison_data['comparisons']:
                    comparison_results.append({
                        'Comparison': f"{comp['group1_name']} vs {comp['group2_name']}",
                        'Case': comp['case_name'],
                        'Î”Mean': comp['delta_mean'],
                        'SD_diff': comp['sd_diff'],
                        '95%_CI_Lower': comp['ci_lower'],
                        '95%_CI_Upper': comp['ci_upper'],
                        'Cohens_d': comp['cohens_d'],
                        'Hedges_g': comp['hedges_g'],
                        'P_Value': comp['p_value'],
                        'Significant': 'Yes' if comp['significant'] else 'No',
                        'Interpretation': comp['interpretation']
                    })
                
                df_comparisons = pd.DataFrame(comparison_results)
                df_comparisons.to_excel(writer, sheet_name='ç»„é—´æ¯”è¾ƒç»“æœ', index=False)
            
            # Sheet3: æ•°æ®è´¨é‡åˆ†æ
            quality_data = []
            for group_name, group_analysis in result_data['analysis'].items():
                quality_data.append({
                    'Group': group_name,
                    'Total_Cases': group_analysis['situation_count'],
                    'Min_Level': group_analysis['min_level'],
                    'Conservative_Strategy': 'Yes' if group_analysis['conservative_strategy'] else 'No',
                    'Precision': self._get_precision_description(group_analysis['min_level'])
                })
            
            if quality_data:
                df_quality = pd.DataFrame(quality_data)
                df_quality.to_excel(writer, sheet_name='æ•°æ®è´¨é‡åˆ†æ', index=False)
            
            # Sheet4: è¯¦ç»†åˆ†æ
            detail_data = []
            for group_name, group_analysis in result_data['analysis'].items():
                for situation in group_analysis['situations']:
                    detail_data.append({
                        'Group': group_name,
                        'Case': situation['situation_name'],
                        'Available_Data': ', '.join(situation['available_data']),
                        'Original_Level': situation['data_level'],
                        'Used_Level': group_analysis['min_level'],
                        'Conservative_Applied': 'Yes' if situation['data_level'] > group_analysis['min_level'] else 'No'
                    })
            
            if detail_data:
                df_detail = pd.DataFrame(detail_data)
                df_detail.to_excel(writer, sheet_name='è¯¦ç»†åˆ†æ', index=False)
            
            # Sheet5: æ‘˜è¦ä¿¡æ¯
            summary = result_data['summary']
            summary_data = [
                {'é¡¹ç›®': 'æ€»ç»„æ•°', 'å€¼': summary['total_groups']},
                {'é¡¹ç›®': 'æ€»æƒ…å†µæ•°', 'å€¼': summary['total_situations']},
                {'é¡¹ç›®': 'ä¿å®ˆä¼°è®¡ç»„æ•°', 'å€¼': summary['conservative_groups']},
                {'é¡¹ç›®': 'æ•´ä½“æœ€ä½ç­‰çº§', 'å€¼': summary['overall_min_level']},
                {'é¡¹ç›®': 'ç²¾åº¦ä¼°è®¡', 'å€¼': summary['precision_estimate']},
                {'é¡¹ç›®': 'å¤„ç†æ—¶é—´', 'å€¼': datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
            ]
            
            # å¦‚æœæœ‰æ¯”è¾ƒæ•°æ®ï¼Œæ·»åŠ æ¯”è¾ƒæ‘˜è¦
            if comparison_data:
                summary_data.extend([
                    {'é¡¹ç›®': 'æ€»æ¯”è¾ƒæ•°', 'å€¼': comparison_data['total_comparisons']},
                    {'é¡¹ç›®': 'æ˜¾è‘—æ¯”è¾ƒæ•°', 'å€¼': comparison_data['significant_comparisons']},
                    {'é¡¹ç›®': 'æ¯”è¾ƒç±»å‹', 'å€¼': comparison_data['comparison_type']},
                    {'é¡¹ç›®': 'ç½®ä¿¡æ°´å¹³', 'å€¼': f"{comparison_data['confidence_level']*100}%"}
                ])
            
            df_summary = pd.DataFrame(summary_data)
            df_summary.to_excel(writer, sheet_name='æ‘˜è¦ä¿¡æ¯', index=False)
            
            # å¦‚æœæœ‰å»ºè®®ï¼Œæ·»åŠ åˆ°æ‘˜è¦
            if summary['recommendations']:
                rec_data = [{'å»ºè®®': rec} for rec in summary['recommendations']]
                df_rec = pd.DataFrame(rec_data)
                df_rec.to_excel(writer, sheet_name='æ”¹è¿›å»ºè®®', index=False)
        
        return final_filepath
    
    def save_to_csv(self, result_data: Dict, output_dir: str = "results", 
                   base_filename: str = "iqr_results_summary.csv") -> str:
        """ä¿å­˜æ‘˜è¦ç»“æœåˆ°CSVæ–‡ä»¶"""
        self.ensure_results_dir(output_dir)
        filepath = os.path.join(output_dir, base_filename)
        final_filepath = self.get_available_filename(filepath)
        
        # å‡†å¤‡CSVæ•°æ®
        csv_data = []
        for group_name, group_results in result_data['results'].items():
            for result in group_results:
                csv_data.append({
                    'Group': group_name,
                    'Case': result['situation_name'],
                    'Mean': round(result['mean'], 4),
                    'SD': round(result['sd'], 4),
                    'Sample_Size': result['sample_size'],
                    'Data_Level': result['used_level'],
                    'Method': result['method_used'],
                    'Conservative_Estimate': 'Yes' if result.get('conservative_estimate') else 'No'
                })
        
        # å†™å…¥CSV
        if csv_data:
            df = pd.DataFrame(csv_data)
            df.to_csv(final_filepath, index=False, encoding='utf-8')
        
        return final_filepath
    
    def save_results(self, result_data: Dict, output_dir: str = "results", 
                    base_name: str = "iqr_results", save_csv: bool = True) -> Dict[str, str]:
        """ä¿å­˜æ‰€æœ‰ç»“æœæ–‡ä»¶"""
        saved_files = {}
        
        # ä¿å­˜Excelæ–‡ä»¶
        excel_filename = f"{base_name}.xlsx"
        excel_path = self.save_to_excel(result_data, output_dir, excel_filename)
        saved_files['excel'] = excel_path
        
        # ä¿å­˜CSVæ‘˜è¦æ–‡ä»¶
        if save_csv:
            csv_filename = f"{base_name}_summary.csv"
            csv_path = self.save_to_csv(result_data, output_dir, csv_filename)
            saved_files['csv'] = csv_path
        
        return saved_files

def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(
        description='CSVæ‰¹é‡ç®±çº¿å›¾æ•°æ®è½¬æ¢å·¥å…·',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨æ­¥éª¤:
  1. ç”Ÿæˆæ¨¡æ¿: python csv_converter.py --generate-template
  2. å¡«å†™æ•°æ®: ç¼–è¾‘ template.csvï¼Œä¿å­˜ä¸º data.csv
  3. è½¬æ¢æ•°æ®: python csv_converter.py --convert data.csv
  
ç¤ºä¾‹:
  python csv_converter.py --generate-template --situations 6
  python csv_converter.py --convert data.csv --verbose
  python csv_converter.py --convert data.csv --output-dir my_results
  python csv_converter.py --convert data.csv --output-name analysis_2025 --no-csv
        """
    )
    
    parser.add_argument('--generate-template', action='store_true', help='ç”ŸæˆCSVæ¨¡æ¿')
    parser.add_argument('--situations', type=int, default=4, help='æ¨¡æ¿ä¸­çš„æƒ…å†µæ•°é‡ (é»˜è®¤: 4)')
    parser.add_argument('--convert', help='è½¬æ¢CSVæ–‡ä»¶')
    parser.add_argument('--verbose', action='store_true', help='è¯¦ç»†è¾“å‡º')
    parser.add_argument('--json', action='store_true', help='JSONæ ¼å¼è¾“å‡º')
    parser.add_argument('--output-dir', default='results', help='è¾“å‡ºç›®å½• (é»˜è®¤: results)')
    parser.add_argument('--output-name', default='iqr_results', help='è¾“å‡ºæ–‡ä»¶åŸºç¡€åç§° (é»˜è®¤: iqr_results)')
    parser.add_argument('--no-csv', action='store_true', help='ä¸ç”ŸæˆCSVæ‘˜è¦æ–‡ä»¶')
    
    # ç»„é—´æ¯”è¾ƒåŠŸèƒ½
    parser.add_argument('--compare-groups', action='store_true', help='å¯ç”¨ç»„é—´æ¯”è¾ƒåŠŸèƒ½')
    parser.add_argument('--comparison-type', choices=['all', 'intervention-baseline', 'pairwise'], 
                       default='intervention-baseline', help='æ¯”è¾ƒç±»å‹ (é»˜è®¤: intervention-baseline)')
    parser.add_argument('--confidence-level', type=float, default=0.95, 
                       help='ç½®ä¿¡æ°´å¹³ (é»˜è®¤: 0.95)')
    parser.add_argument('--meta-analysis-format', action='store_true', 
                       help='ç”ŸæˆMetaåˆ†ææ ‡å‡†æ ¼å¼æ–‡ä»¶')
    
    args = parser.parse_args()
    
    converter = CSVConverter()
    
    try:
        if args.generate_template:
            filename = converter.generate_template(args.situations)
            print(f"âœ“ å·²ç”Ÿæˆæ¨¡æ¿æ–‡ä»¶: {filename}")
            print(f"  æ”¯æŒ {args.situations} ä¸ªæƒ…å†µ")
            print(f"  è¯·å¡«å†™æ•°æ®åä¿å­˜ä¸º data.csv")
            return
        
        if args.convert:
            print("=== CSVç®±çº¿å›¾æ•°æ®è½¬æ¢å·¥å…· ===\n")
            print(f"å¤„ç†æ–‡ä»¶: {args.convert}")
            
            result = converter.convert_csv_data(args.convert, args.verbose)
            
            # æ‰§è¡Œç»„é—´æ¯”è¾ƒï¼ˆå¦‚æœå¯ç”¨ï¼‰
            comparison_data = None
            if args.compare_groups:
                print(f"\næ­£åœ¨æ‰§è¡Œç»„é—´æ¯”è¾ƒåˆ†æ...")
                comparison_data = converter.perform_group_comparisons(
                    result, 
                    args.comparison_type, 
                    args.confidence_level
                )
                
                if not args.json:
                    print_comparison_results(comparison_data, args.verbose)
            
            if args.json:
                output_data = result.copy()
                if comparison_data:
                    output_data['comparisons'] = comparison_data
                print(json.dumps(output_data, indent=2, ensure_ascii=False))
            else:
                print_results(result, args.verbose)
            
            # å¼ºåˆ¶ä¿å­˜ç»“æœ
            print(f"\næ­£åœ¨ä¿å­˜ç»“æœ...")
            try:
                # ä¿å­˜åŸºç¡€ç»“æœï¼ˆåŒ…å«æ¯”è¾ƒæ•°æ®ï¼‰
                excel_filename = f"{args.output_name}.xlsx"
                excel_path = converter.save_to_excel(
                    result, 
                    comparison_data, 
                    args.output_dir, 
                    excel_filename
                )
                
                saved_files = {'excel': excel_path}
                
                # ä¿å­˜CSVæ‘˜è¦æ–‡ä»¶
                if not args.no_csv:
                    csv_filename = f"{args.output_name}_summary.csv"
                    csv_path = converter.save_to_csv(result, args.output_dir, csv_filename)
                    saved_files['csv'] = csv_path
                
                # ç”ŸæˆMetaåˆ†ææ ¼å¼æ–‡ä»¶ï¼ˆå¦‚æœå¯ç”¨ï¼‰
                if args.meta_analysis_format and comparison_data:
                    meta_files = converter.generate_meta_analysis_formats(
                        result, comparison_data, args.output_dir
                    )
                    saved_files.update(meta_files)
                
                print(f"\nâœ“ ç»“æœå·²ä¿å­˜:")
                print(f"  ğŸ“Š è¯¦ç»†ç»“æœ: {saved_files['excel']}")
                if 'csv' in saved_files:
                    print(f"  ğŸ“‹ æ‘˜è¦ç»“æœ: {saved_files['csv']}")
                
                if args.meta_analysis_format and comparison_data:
                    print(f"  ğŸ“ˆ Metaåˆ†ææ ¼å¼:")
                    for format_name, file_path in meta_files.items():
                        print(f"    - {format_name}: {file_path}")
                
                print(f"\næ–‡ä»¶è¯´æ˜:")
                print(f"- Excelæ–‡ä»¶åŒ…å«å®Œæ•´åˆ†æå’Œå¤šä¸ªå·¥ä½œè¡¨")
                if comparison_data:
                    print(f"- åŒ…å«ç»„é—´æ¯”è¾ƒç»“æœå’Œç½®ä¿¡åŒºé—´åˆ†æ")
                if 'csv' in saved_files:
                    print(f"- CSVæ–‡ä»¶ä¸ºç®€åŒ–æ‘˜è¦ï¼Œä¾¿äºå¯¼å…¥å…¶ä»–è½¯ä»¶")
                if args.meta_analysis_format and comparison_data:
                    print(f"- Metaåˆ†ææ ¼å¼æ–‡ä»¶å¯ç›´æ¥å¯¼å…¥RevManã€Rç­‰è½¯ä»¶")
                
            except Exception as save_error:
                print(f"âš ï¸  ä¿å­˜æ–‡ä»¶æ—¶å‡ºç°é—®é¢˜: {save_error}")
                print(f"ç»“æœå·²åœ¨å±å¹•ä¸Šæ˜¾ç¤ºï¼Œè¯·æ‰‹åŠ¨ä¿å­˜")
        
        else:
            parser.print_help()
    
    except Exception as e:
        print(f"é”™è¯¯: {e}", file=sys.stderr)
        sys.exit(1)

def print_results(result: Dict, verbose: bool = False):
    """æ‰“å°ç»“æœ"""
    analysis = result['analysis']
    results = result['results']
    summary = result['summary']
    
    print("=" * 60)
    print("æ•°æ®è´¨é‡åˆ†æ")
    print("=" * 60)
    
    for group_name, group_analysis in analysis.items():
        print(f"\n{group_name}:")
        for situation in group_analysis['situations']:
            level_desc = {-1: "âŒ ä¸å®Œæ•´", 0: "âš ï¸  ç­‰çº§0", 1: "âœ“ ç­‰çº§1", 2: "âœ“âœ“ ç­‰çº§2"}
            print(f"  {situation['situation_name']}: {level_desc.get(situation['data_level'], '?')} "
                  f"({', '.join(situation['available_data'])})")
        
        if group_analysis['conservative_strategy']:
            print(f"  â†’ é‡‡ç”¨ä¿å®ˆä¼°è®¡: ç­‰çº§{group_analysis['min_level']}")
    
    print(f"\næ•´ä½“ç²¾åº¦: {summary['precision_estimate']}")
    
    if summary['recommendations']:
        print(f"\næ”¹è¿›å»ºè®®:")
        for rec in summary['recommendations']:
            print(f"  â€¢ {rec}")
    
    print("\n" + "=" * 60)
    print("è½¬æ¢ç»“æœ")
    print("=" * 60)
    
    for group_name, group_results in results.items():
        print(f"\n{group_name}:")
        for result in group_results:
            conservative_mark = " (ä¿å®ˆä¼°è®¡)" if result.get('conservative_estimate') else ""
            print(f"  {result['situation_name']}: Mean={result['mean']:.3f}, "
                  f"SD={result['sd']:.3f}{conservative_mark}")
            if verbose:
                print(f"    æ–¹æ³•: {result['method_used']}, ç­‰çº§: {result['used_level']}")

def print_comparison_results(comparison_data: Dict, verbose: bool = False):
    """æ‰“å°ç»„é—´æ¯”è¾ƒç»“æœ"""
    print("\n" + "=" * 60)
    print("ç»„é—´æ¯”è¾ƒåˆ†æ")
    print("=" * 60)
    
    print(f"\næ¯”è¾ƒç±»å‹: {comparison_data['comparison_type']}")
    print(f"ç½®ä¿¡æ°´å¹³: {comparison_data['confidence_level']*100}%")
    print(f"æ€»æ¯”è¾ƒæ•°: {comparison_data['total_comparisons']}")
    print(f"æ˜¾è‘—æ¯”è¾ƒæ•°: {comparison_data['significant_comparisons']}")
    
    if comparison_data['comparisons']:
        print(f"\nè¯¦ç»†æ¯”è¾ƒç»“æœ:")
        print("-" * 80)
        
        for comp in comparison_data['comparisons']:
            print(f"\nğŸ“Š {comp['group1_name']} vs {comp['group2_name']} ({comp['case_name']})")
            print(f"   Î”Mean = {comp['delta_mean']:.4f}")
            print(f"   SD_diff = {comp['sd_diff']:.4f}")
            print(f"   95% CI: [{comp['ci_lower']:.4f}, {comp['ci_upper']:.4f}]")
            print(f"   Cohen's d = {comp['cohens_d']:.4f}")
            print(f"   På€¼ = {comp['p_value']:.4f}")
            
            # æ˜¾è‘—æ€§æ ‡è®°
            if comp['significant']:
                print(f"   âœ“ {comp['interpretation']}")
            else:
                print(f"   â—‹ {comp['interpretation']}")
            
            if verbose:
                print(f"   è¯¦ç»†ä¿¡æ¯:")
                print(f"     - Hedges' g = {comp['hedges_g']:.4f}")
                print(f"     - tç»Ÿè®¡é‡ = {comp['t_statistic']:.4f}")
                print(f"     - è‡ªç”±åº¦ = {comp['degrees_of_freedom']}")
                print(f"     - ç»„1æ•°æ®: Mean={comp['group1_data']['mean']:.3f}, SD={comp['group1_data']['sd']:.3f}, N={comp['group1_data']['sample_size']}")
                print(f"     - ç»„2æ•°æ®: Mean={comp['group2_data']['mean']:.3f}, SD={comp['group2_data']['sd']:.3f}, N={comp['group2_data']['sample_size']}")
    
    print("\n" + "=" * 60)
    print("æ¯”è¾ƒç»“æœæ‘˜è¦")
    print("=" * 60)
    
    # æŒ‰æ˜¾è‘—æ€§åˆ†ç»„æ˜¾ç¤º
    significant_comps = [c for c in comparison_data['comparisons'] if c['significant']]
    non_significant_comps = [c for c in comparison_data['comparisons'] if not c['significant']]
    
    if significant_comps:
        print(f"\nâœ“ æ˜¾è‘—å·®å¼‚ ({len(significant_comps)}ä¸ª):")
        for comp in significant_comps:
            direction = "â†‘" if comp['delta_mean'] > 0 else "â†“"
            print(f"  {direction} {comp['case_name']}: Î”Mean={comp['delta_mean']:.3f} (p={comp['p_value']:.3f})")
    
    if non_significant_comps:
        print(f"\nâ—‹ æ— æ˜¾è‘—å·®å¼‚ ({len(non_significant_comps)}ä¸ª):")
        for comp in non_significant_comps:
            print(f"    {comp['case_name']}: Î”Mean={comp['delta_mean']:.3f} (p={comp['p_value']:.3f})")

if __name__ == "__main__":
    main()
